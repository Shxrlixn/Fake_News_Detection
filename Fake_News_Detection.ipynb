{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJ_TqzQ-uyyJ"
   },
   "source": [
    "# <center>FAKE NEWS DETECTION</center>\n",
    "<center>BY: SHERLIEN MOLLY D</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rw3ehovqlJAZ"
   },
   "source": [
    "1. Import Libraries\n",
    "In this step, we import all the necessary libraries for data manipulation, text processing, and machine learning model building. This includes essential libraries like `pandas`, `nltk` for natural language processing, `TfidfVectorizer` for converting text into numerical features, and `LogisticRegression` for model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LKXl8tUlYBf"
   },
   "source": [
    "Explanation:\n",
    " - pandas: For handling data in tabular format.\n",
    " - nltk: For text preprocessing, including stopwords and tokenization.\n",
    " - re: Regular expressions for text cleaning.\n",
    " - sklearn: Machine learning functions for splitting data, vectorizing text, training models, and evaluating performance.\n",
    " - seaborn and matplotlib: For data visualization and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['usage: install [-bCcpSsv] [-B suffix] [-f flags] [-g group] [-m mode]',\n",
       " '               [-o owner] file1 file2',\n",
       " '       install [-bCcpSsv] [-B suffix] [-f flags] [-g group] [-m mode]',\n",
       " '               [-o owner] file1 ... fileN directory',\n",
       " '       install -d [-v] [-g group] [-m mode] [-o owner] directory ...']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!! install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YtxPR_fPldrs",
    "outputId": "73eee1b4-ef41-42ec-8168-71593a43b2f2"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "#Download necessary resources from nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ij5d9OehlxCP"
   },
   "source": [
    "2. Load the Data\n",
    "We load two datasets: one containing fake news and another with true news. After loading the datasets, we label the data as fake (0) or true (1) and combine the two into a single DataFrame for ease of processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c-M6-hwWmRLU",
    "outputId": "7fb64927-5138-4e82-b11e-e83a27548969"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44898 entries, 0 to 44897\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    44898 non-null  object\n",
      " 1   text     44898 non-null  object\n",
      " 2   subject  44898 non-null  object\n",
      " 3   date     44898 non-null  object\n",
      " 4   label    44898 non-null  int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 1.7+ MB\n",
      "None\n",
      "                                               title  \\\n",
      "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
      "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
      "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
      "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
      "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
      "\n",
      "                                                text subject  \\\n",
      "0  Donald Trump just couldn t wish all Americans ...    News   \n",
      "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
      "2  On Friday, it was revealed that former Milwauk...    News   \n",
      "3  On Christmas day, Donald Trump announced that ...    News   \n",
      "4  Pope Francis used his annual Christmas Day mes...    News   \n",
      "\n",
      "                date  label  \n",
      "0  December 31, 2017      0  \n",
      "1  December 31, 2017      0  \n",
      "2  December 30, 2017      0  \n",
      "3  December 29, 2017      0  \n",
      "4  December 25, 2017      0  \n"
     ]
    }
   ],
   "source": [
    "fake_df = pd.read_csv('Fake.csv')\n",
    "true_df = pd.read_csv('True.csv')\n",
    "\n",
    "#Add a label column to both datasets: 0 for fake, 1 for true\n",
    "fake_df['label'] = 0\n",
    "true_df['label'] = 1\n",
    "\n",
    "#Combine both datasets\n",
    "combined_df = pd.concat([fake_df, true_df], ignore_index=True)\n",
    "\n",
    "#View basic info\n",
    "print(combined_df.info())\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eC-tUuPomdN1"
   },
   "source": [
    "Explanation:\n",
    " - We add a `label` column to distinguish between fake (0) and true (1) news articles.\n",
    " - The `concat` function combines the two datasets vertically (row-wise).\n",
    " - Displaying the first few rows and data info ensures that the data is correctly loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIXg6InvngIv"
   },
   "source": [
    "3. Data Preprocessing\n",
    "\n",
    " A. Text Cleaning\n",
    "This step is critical as raw text can be noisy and contain irrelevant information such as punctuation, numbers, or stopwords. We clean the text by:\n",
    "- Lowercasing the text for uniformity.\n",
    "- Removing punctuation and numbers.\n",
    "- Tokenizing the text (breaking it into individual words).\n",
    "- Removing common stopwords (e.g., \"the\", \"and\") which do not contribute meaningful information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "BD1QP4BAnwhE",
    "outputId": "496e7068-1f22-414d-f2bf-3a9a272bbc3c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(cleaned_tokens)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#Apply the cleaning function to the 'text' column\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m combined_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#Check cleaned data\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(combined_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m, in \u001b[0;36mclean_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      4\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)   \u001b[38;5;66;03m#Remove numbers\u001b[39;00m\n\u001b[1;32m      5\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)   \u001b[38;5;66;03m#Remove punctuation and special characters\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m(text)   \u001b[38;5;66;03m#Tokenize\u001b[39;00m\n\u001b[1;32m      7\u001b[0m cleaned_tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)]   \u001b[38;5;66;03m#Remove stopwords\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(cleaned_tokens)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "#Function to clean the text\n",
    "def clean_text(text):\n",
    "    text = text.lower()   #Lowercase\n",
    "    text = re.sub(r'\\d+', '', text)   #Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)   #Remove punctuation and special characters\n",
    "    tokens = word_tokenize(text)   #Tokenize\n",
    "    cleaned_tokens = [word for word in tokens if word not in stopwords.words('english')]   #Remove stopwords\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "#Apply the cleaning function to the 'text' column\n",
    "combined_df['cleaned_text'] = combined_df['text'].apply(clean_text)\n",
    "\n",
    "#Check cleaned data\n",
    "print(combined_df[['title', 'cleaned_text', 'label']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7LQdIflnzq_"
   },
   "source": [
    "Explanation:\n",
    " - The `clean_text` function removes noise from the text data, helping the machine learning model focus on the most relevant words.\n",
    " - Applying the function to the 'text' column creates a new 'cleaned_text' column.\n",
    " - Cleaned text is displayed for verification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H11DzT0to7sg"
   },
   "source": [
    "4. Exploratory Data Analysis (EDA)\n",
    "Exploratory Data Analysis (EDA) helps us understand the structure of the data and the distribution of features. Here, we examine the distribution of fake vs true articles and common word frequencies in both categories.\n",
    "\n",
    " A. Distribution of Fake vs True Articles\n",
    "We visualize the number of fake and true news articles using a bar chart to get an overview of the class distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_r2qIqno_er"
   },
   "outputs": [],
   "source": [
    "#Countplot for labels\n",
    "sns.countplot(x='label', data=combined_df)\n",
    "plt.title('Distribution of Fake vs True News')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuUj6ZeKqSsO"
   },
   "source": [
    "Explanation:\n",
    " - This count plot shows how balanced the dataset is. A balanced dataset ensures the model does not favor one class over the other.\n",
    " - Helps understand the ratio of fake to true news articles, which impacts the evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IthhH9aOqV-U"
   },
   "source": [
    "B. Common Words in Fake vs True News\n",
    "We compute and display the most common words in fake and true news to gain insights into which terms are frequent in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLmduJAZqg5z"
   },
   "outputs": [],
   "source": [
    "#Function to get common words\n",
    "def get_most_common_words(df, label, n=20):\n",
    "    all_words = ' '.join(df[df['label'] == label]['cleaned_text']).split()\n",
    "    word_freq = Counter(all_words)\n",
    "    return word_freq.most_common(n)\n",
    "\n",
    "#Get common words for fake and true news\n",
    "common_fake_words = get_most_common_words(combined_df, label=0)\n",
    "common_true_words = get_most_common_words(combined_df, label=1)\n",
    "\n",
    "print('Most common words in fake news:', common_fake_words)\n",
    "print('Most common words in true news:', common_true_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSAC_nsOqqjv"
   },
   "source": [
    "Explanation:\n",
    " - `get_most_common_words` function computes word frequencies for both fake and true news.\n",
    " - This gives us an idea of the different vocabularies used in fake and true news articles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9laCt8zqwXB"
   },
   "source": [
    "C. Length of News Articles\n",
    "We visualize the distribution of article lengths for both fake and true news to check if there’s a difference in the size of articles between these two categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "urN3NH0wrY9g"
   },
   "outputs": [],
   "source": [
    "#Compute text length\n",
    "combined_df['text_length'] = combined_df['cleaned_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "#Boxplot for text lengths\n",
    "sns.boxplot(x='label', y='text_length', data=combined_df)\n",
    "plt.title('Distribution of Article Lengths by Fake vs True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XexSA_OGriK_"
   },
   "source": [
    "\n",
    "Explanation:\n",
    " - Boxplot shows the distribution of the number of words in fake and true articles.\n",
    " - This helps in understanding whether fake or true news articles are typically longer or shorter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeDV2Yy5rxTz"
   },
   "source": [
    "5. Text Vectorization (TF-IDF)\n",
    "We convert the cleaned text into numerical features using TF-IDF (Term Frequency-Inverse Document Frequency). This method weighs terms by how frequently they occur in each document while reducing the weight of commonly occurring terms across the entire corpus (e.g., \"the\", \"is\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oObzyuMIsJ_9"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Use TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "#Fit and transform the text data\n",
    "X = tfidf.fit_transform(combined_df['cleaned_text']).toarray()\n",
    "y = combined_df['label']\n",
    "\n",
    "#Check shape of feature matrix\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPL3f_T7sQtW"
   },
   "source": [
    "Explanation:\n",
    " - TF-IDF converts the text into a numerical feature matrix.\n",
    " - `max_features=5000` limits the number of features to the 5000 most important words.\n",
    " - The feature matrix `X` is ready for model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxQ1BO4Ssbon"
   },
   "source": [
    "6. Train-Test Split\n",
    "We split the data into training and testing sets. This ensures that we train the model on one part of the data and test its performance on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzfIQiTMskpx"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BygB6ZQksqj3"
   },
   "source": [
    "Explanation:\n",
    " - The data is split into 80% for training and 20% for testing.\n",
    " - `random_state=42` ensures that the split is reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYBMs3_qsy-f"
   },
   "source": [
    "7. Model Training: Logistic Regression\n",
    "We use Logistic Regression, a commonly used algorithm for binary classification tasks. It models the probability that a given input belongs to a certain class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ej7jxFqGtNEi"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Initialize Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "#Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Make predictions\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQZduiBmtRWf"
   },
   "source": [
    "Explanation:\n",
    " - We train the logistic regression model on the training data.\n",
    " - Once trained, the model is used to predict the class (fake or true) for the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "801KwzPdtUTg"
   },
   "source": [
    " 8. Model Evaluation\n",
    "\n",
    " A. Accuracy and Classification Report\n",
    "The accuracy and classification report give a detailed breakdown of how well the model performs. The classification report shows precision, recall, and F1-score for both classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LMCzaCBitiWy"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "#Classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHW_z4AotrGZ"
   },
   "source": [
    "Explanation:\n",
    " - Accuracy is the percentage of correctly classified news articles.\n",
    " - The classification report provides precision, recall, and F1-score for both fake and true news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuTC8lDYt41k"
   },
   "source": [
    "B. Confusion Matrix\n",
    "We visualize the confusion matrix to see how well the model distinguishes between fake and true articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UuaTUBK2uKfB"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "#Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    " #Plot the confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'True'], yticklabels=['Fake', 'True'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vgk5OSvLuQae"
   },
   "source": [
    "Explanation:\n",
    " - The confusion matrix helps visualize the number of correct and incorrect predictions made by the model.\n",
    " - It shows how many fake news articles were misclassified as true and vice versa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4-QelLeuV46"
   },
   "source": [
    "9. Experiment with Other Models (Optional)\n",
    "You can experiment with other machine learning models, like Random Forest, to see if a more complex algorithm performs better than Logistic Regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8pBkgRHsueeQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Initialize Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "#Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "#Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "#Evaluate Random Forest model\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f'Random Forest Accuracy: {accuracy_rf * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ev9jdAv5jm9t"
   },
   "source": [
    " Explanation:\n",
    " - Random Forest is an ensemble learning method that combines multiple decision trees to improve accuracy and robustness.\n",
    " - Training and testing this model allows for comparison with Logistic Regression.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " Conclusion\n",
    "- The project demonstrates the full pipeline for a fake news detection system using machine learning.\n",
    "- We started with raw text data, cleaned and preprocessed it, and then applied machine learning algorithms to classify the articles.\n",
    "- Evaluation metrics such as accuracy, precision, recall, and the confusion matrix help assess the model's performance.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
